var documenterSearchIndex = {"docs":
[{"location":"references/#Referenes","page":"References","title":"Referenes","text":"","category":"section"},{"location":"references/#Credits","page":"References","title":"Credits","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"This package and the manual were strongly influenced by Hansen (2000). The image for the logo is a picture of Andrey Nikolayevich Tikhonov taken from Wikipedia. The credit for introducing the regularized solution is shared with Twomey (1963).","category":"page"},{"location":"references/#Bibliography","page":"References","title":"Bibliography","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Baart, M. L. (1982) The use of auto-correlation for pseudo-rank determination in noisy ill-conditioned linear least-squares problems, IMA, J. Numer. Anal. 2, 241-247.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Eldén, L. (1977) Algorithms for the regularization of ill-conditioned least squares problems, BIT 17, 134–145, DOI:10.1007/BF01932285.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Golub, G. H., Heath, M. and Wahba, G. (1979) Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter,  Technometrics 21(2), 215–223, DOI:10.1080/00401706.1979.10489751.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Hansen, P. C. (1998) 2. Decompositions and Other Tools, in Rank-Deficient and Discrete Ill-Posed Problems, 19-44, DOI:10.1137/1.9780898719697.ch2.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Hansen, P. C. (2000) The L-Curve and its Use in the Numerical Treatment of Inverse Problems, in Computational Inverse Problems in Electrocardiology, ed. P. Johnston, Advances in Computational Bioengineering, 119–142, WIT Press.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Hansen, P. C. (2008) Regularization Tools, A Matlab Package for Analysis and Solution of Discrete Ill-Posed Problems Version 4.1 for Matlab 7.3, http://www.imm.dtu.dk/~pcha/Regutools/RTv4manual.pdf. ","category":"page"},{"location":"references/","page":"References","title":"References","text":"Huckle, T. and Sedlacek, M. (2012) Data Based Regularization Matrices for the Tikhonov-PhillipsRegularization, Proc. Appl. Math. Mech., 12, 643 – 644,  DOI:10.1002/pamm.201210310.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Lira, M, Iyer, R., Trinidade, A., Howle, V. (2016) QR versus Cholesky: A probabilistic analysis, International Journal of Numerical Analysis and Modeling, 13(1), 114-121.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Twomey, S. (1963) On the numerical solution of Fredholm integral equations of the first kind by inversion of the linear system produced by quadrature, Journal of the ACM, 19(1963), 97–101, DOI:10.1145/321150.321157.","category":"page"},{"location":"library/#Data-Types","page":"Library","title":"Data Types","text":"","category":"section"},{"location":"library/#RegularizationProblem","page":"Library","title":"RegularizationProblem","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"RegularizationProblem","category":"page"},{"location":"library/#RegularizationTools.RegularizationProblem","page":"Library","title":"RegularizationTools.RegularizationProblem","text":"RegularizationProblem\n\nThis data type contains the cached matrices used in the inversion. The problem is  initialized using the constructor setupRegularizationProblem with the design matrix  A and the the Tikhonv matrix L as inputs. The hat quantities, e.g. Ā, is the calculated design matrix in standard form. ĀĀ, Āᵀ, F̄ are precomputed to speed up repeating inversions with different data. L⁺ₐ is cached to speed up the repeated conversion of  data to_standard_form and to_general_form\n\nĀ::Matrix{Float64}     # Standard form of design matrix\nA::Matrix{Float64}     # General form of the design matrix (n×p)\nL::Matrix{Float64}     # Smoothing matrix (n×p)\nĀĀ::Matrix{Float64}    # Cached value of Ā'Ā for performance\nĀᵀ::Matrix{Float64}    # Cached value of Ā' for performance\nF̄::SVD                 # Cached SVD decomposition of Ā \nIₙ::Matrix{Float64}    # Cached identity matrix n×n\nIₚ::Matrix{Float64}    # Cached identity matrix p×p\nL⁺ₐ::Matrix{Float64}   # Cached A-weighted generalized inverse of L(standard-form conversion)\n\n\n\n\n\n","category":"type"},{"location":"library/#RegularizatedSolution","page":"Library","title":"RegularizatedSolution","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"RegularizedSolution","category":"page"},{"location":"library/#RegularizationTools.RegularizedSolution","page":"Library","title":"RegularizationTools.RegularizedSolution","text":"RegularizatedSolution\n\nData tpye to store the optimal solution x of the inversion. λ is the optimal λ used  solution is the raw output from the Optim search.\n\nx::AbstractVector\nλ::AbstractFloat\nsolution::Optim.UnivariateOptimizationResults\n\n\n\n\n\n","category":"type"},{"location":"library/#Domain","page":"Library","title":"Domain","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"Domain","category":"page"},{"location":"library/#RegularizationTools.Domain","page":"Library","title":"RegularizationTools.Domain","text":"Domain{T1<:Any,T2<:Number,T3<:Any}\n\nFunctor to map from a domain characterized by a list of setpoints [s], each  associated with a list of numerical values [x] to a query value q. \n\ns::AbstractVector{T1}\nx::AbstractVector{T2}\nq::T3\n\n\n\n\n\n","category":"type"},{"location":"library/#Constructor-Functions","page":"Library","title":"Constructor Functions","text":"","category":"section"},{"location":"library/#Tikhonov-Matrix","page":"Library","title":"Tikhonov Matrix","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"Γ","category":"page"},{"location":"library/#RegularizationTools.Γ","page":"Library","title":"RegularizationTools.Γ","text":"Γ(A::AbstractMatrix, order::Int)\n\nReturn the smoothing matrix L for zero, first and second order Tikhonov regularization  based on the size of design matrix A. Order can be 0, 1 or 2.\n\nL = Γ(A, 1)\n\n\n\n\n\n","category":"function"},{"location":"library/#setupRegularizationProblem","page":"Library","title":"setupRegularizationProblem","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"setupRegularizationProblem","category":"page"},{"location":"library/#RegularizationTools.setupRegularizationProblem","page":"Library","title":"RegularizationTools.setupRegularizationProblem","text":"setupRegularizationProblem(A::AbstractMatrix, order::Int)\n\nPrecompute matrices to initialize Reguluarization Problem based on design matrix A and  zeroth, first, or second order difference operator. See Hanson (1998) and source code for details.\n\nExample Usage\n\nΨ = setupRegularizationProblem(A, 0) # zeroth order problem\nΨ = setupRegularizationProblem(A, 2) # second order problem\n\n\n\n\n\nsetupRegularizationProblem(A::AbstractMatrix, L::AbstractMatrix)\n\nPrecompute matrices to initialize Reguluarization Problem based on design matrix and  Tikhonov smoothing matrix. See Hansen (1998, Eq. 2.35)\n\nExample Usage\n\nΨ = setupRegularizationProblem(A, L) \n\n\n\n\n\n","category":"function"},{"location":"library/#to_standard_form","page":"Library","title":"to_standard_form","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"to_standard_form","category":"page"},{"location":"library/#RegularizationTools.to_standard_form","page":"Library","title":"RegularizationTools.to_standard_form","text":"to_standard_form(Ψ::RegularizationProblem, b::AbstractVector)\n\nConverts vector b to standard form using (Hansen, 1998)\n\nExample Usage (Regular Syntax)\n\nb̄ = to_standard_form(Ψ, b)\n\nExample Usage (Lazy Syntax)\n\nb̄ = @>> b to_standard_form(Ψ)\n\n\n\n\n\nto_standard_form(Ψ::RegularizationProblem, b::AbstractVector, x₀::AbstractVector)\n\nConverts vector b and x₀ to standard form using (Hansen, 1998)\n\nExample Usage (Regular Syntax)\n\nb̄ = to_standard_form(Ψ, b, x₀)\n\n\n\n\n\n","category":"function"},{"location":"library/#to_general_form","page":"Library","title":"to_general_form","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"to_general_form","category":"page"},{"location":"library/#RegularizationTools.to_general_form","page":"Library","title":"RegularizationTools.to_general_form","text":"to_general_form(Ψ::RegularizationProblem, b::AbstractVector, x̄::AbstractVector)\n\nConverts solution bar rm x computed in standard form back to general form  rm x using (Hansen, 1998). Solution is truncated to regularized space, given by the matrix L. If L is p × n and p < n, then only the solution 1:p is valid. The remaining  parameters can be estiamted from the least-squares solution if needed.\n\nrm x=rm bf L^+_Abarx\n\nwhere the matrices and vectors are defined in RegularizationProblem\n\nExample Usage (Regular Syntax)\n\nx = to_general_form(Ψ, b, x̄) \n\nExample Usage (Lazy Syntax)\n\nx = @>> x̄ to_general_form(Ψ, b) \n\n\n\n\n\n","category":"function"},{"location":"library/#Solvers","page":"Library","title":"Solvers","text":"","category":"section"},{"location":"library/#solve","page":"Library","title":"solve","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"solve","category":"page"},{"location":"library/#RegularizationTools.solve","page":"Library","title":"RegularizationTools.solve","text":"solve(Ψ::RegularizationProblem, b̄::AbstractVector, λ::AbstractFloat)\n\nCompute the Tikhonov solution for problem Ψ in standard form for regularization parameter λ and using zero as initial guess. Returns a vector rm bar x_lambda. \n\nrm x_lambda=left(rm bf bar A^Trm bf bar A+lambda^2rm bf Iright)^-1 \nrm bf bar A^Trm bar b \n\nExample Usage (Standard Syntax)\n\n# A is a Matrix and b is a response vector. \nΨ = setupRegularizationProblem(A, 1)     # Setup problem\nb̄ = to_standard_form(Ψ, b)               # Convert to standard form\nx̄ = solve(A, b̄, 0.5)                     # Solve the equation\nx = to_general_form(Ψ, b, x̄)             # Convert back to general form\n\nExample Usage (Lazy Syntax)\n\n# A is a Matrix and b is a response vector. \nΨ = setupRegularizationProblem(A, 1)     # Setup problem\nb̄ = @>> b to_standard_form(Ψ)            # Convert to standard form\nx̄ = solve(A, b̄, 0.5)                     # Solve the equation\nx = @>> x̄ to_general_form(Ψ, b)          # Convert back to general form\n\n\n\n\n\nsolve(Ψ::RegularizationProblem, b̄::AbstractVector, x̄₀::AbstractVector, λ::AbstractFloat)\n\nCompute the Tikhonov solution for problem Ψ in standard form for regularization parameter λ and using x̄₀ as initial guess. \n\nrm x_lambda=left(rm bf bar A^Trm bf bar A+lambda^2rm bf Iright)^-1 \nleft(rm bf bar A^Trm bar b + lambda^2 rm bar x_0 right)\n\nExample Usage (Standard Syntax)\n\n# A is a Matrix and b is a response vector. \nΨ = setupRegularizationProblem(A, 2)     # Setup problem\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)       # Convert to standard form\nx̄ = solve(A, b̄, x̄₀, 0.5)                 # Solve the equation\nx = to_general_form(Ψ, b, x̄)             # Convert back to general form\n\n\n\n\n\nfunction solve(\n    Ψ::RegularizationProblem,\n    b::AbstractVector;\n    alg = :gcv_svd,\n    λ₁ = 0.0001,\n    λ₂ = 1000.0,\n)\n\nFind the optimum regularization parameter λ between [λ₁, λ₂] using the algorithm alg. Choices for algorithms are\n\n    :gcv_tr - generalized cross validation using the trace formulation (slow)\n    :gcv_svd - generalized cross validation using the SVD decomposition (fast)\n    :L_curve - L-curve algorithm \n\ntip: Tip\nThe gcv_svd algorithm is fastest and most stable. The L_curve algorithn is sensitive to the upper  and lower bound. Specify narrow upper and lower bounds to obtain a good solution.\n\nThe solve function takes the original data, converts it to standard form, performs the search within the specified bounds and returns a RegularizatedSolution\n\nExample Usage (Standard Syntax)\n\n# A is a Matrix and b is a response vector. \nΨ = setupRegularizationProblem(A, 2)     # Setup problem\nsol = solve(Ψ, b)                        # Solve it\n\nExample Usage (Lazy Syntax)\n\n# A is a Matrix and b is a response vector. \nsol = @> setupRegularizationProblem(A, 1) solve(b)\n\n\n\n\n\nfunction solve(\n    Ψ::RegularizationProblem,\n    b::AbstractVector,\n    x₀::AbstractVector;\n    alg = :gcv_svd,\n    λ₁ = 0.0001,\n    λ₂ = 1000.0,\n)\n\nSame as above, but includes an initial guess x₀. Example Usage (Lazy Syntax)\n\n# A is a Matrix and b is a response vector. \nsol = @> setupRegularizationProblem(A, 1) solve(b, x₀, alg = :L_curve, λ₂ = 10.0)\n\n\n\n\n\nfunction solve(\n    Ψ::RegularizationProblem, \n    b::AbstractVector,\n    lower::AbstractVector, \n    upper::AbstractVector;\n    kwargs...\n)\n\nConstraint minimization of RegularizationProblem Ψ, with observations b and upper and lower bounds for each xᵢ.\n\nThe function computes the algebraic solution using solve(Ψ, b; kwargs...), truncates the solution at the upper and lower bounds and uses this solution as initial condition for the minimization problem using a Least Squares numerical solver. The returned solution is using the regularization parameter λ obtained from the algebraic solution.\n\n\n\n\n\nfunction solve(\n    Ψ::RegularizationProblem, \n    b::AbstractVector,\n    x₀::AbstractVector,\n    lower::AbstractVector, \n    upper::AbstractVector;\n    kwargs...\n)\n\nConstraint minimization of RegularizationProblem Ψ, with observations b, intial  guess x₀ and upper and lower bounds for each xᵢ.\n\nThe function computes the algebraic solution using solve(Ψ, b; kwargs...), truncates the solution at the upper and lower bounds and uses this solution as initial condition for the minimization problem using a Least Squares numerical solver. The returned solution is using the regularization parameter λ obtained from the algebraic solution.\n\n\n\n\n\n","category":"function"},{"location":"library/#Validators","page":"Library","title":"Validators","text":"","category":"section"},{"location":"library/#GCV","page":"Library","title":"GCV","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"gcv_tr","category":"page"},{"location":"library/#RegularizationTools.gcv_tr","page":"Library","title":"RegularizationTools.gcv_tr","text":"gcv_tr(Ψ::RegularizationProblem, b̄::AbstractVector, λ::AbstractFloat)\n\nCompute the Generalized Cross Validation using the trace term. Requires that the  vector b̄ is in standard form.\n\nV(lambda)=fracnleftlVert (bf rm bf bf I-bf bar A_lambda)\nbarrm brightrVert _2^2tr(rm bf I-rm bar bf A_lambda)^2\n\nExample Usage\n\nusing Underscores\n\nΨ = setupRegularizationProblem(A, 1)           # Setup problem\nb̄ = to_standard_form(Ψ, b)                     # Convert to standard form\nVλ = gcv_tr(Ψ, b̄, 0.1)                         # V(λ) single λ value\nVλ = @_ map(gcv_tr(Ψ, b̄, _), [0.1, 1.0, 10.0]) # V(λ) for array of λ\n\n\n\n\n\ngcv_tr(\n    Ψ::RegularizationProblem,\n    b̄::AbstractVector,\n    x̄₀::AbstractVector,\n    λ::AbstractFloat,\n)\n\nCompute the Generalized Cross Validation using the trace term and intial guess.  Requires that the vectors b̄ and x̄₀ are in standard form.\n\nV(lambda)=fracnleftlVert bf rm bf barArm barx_lambda-\nrm barbrightrVert _2^2tr(rm bf I-rm bar bf A_lambda)^2\n\nExample Usage\n\nusing Underscores\n\nΨ = setupRegularizationProblem(A, 1)               # Setup problem\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)                 # Convert to standard form\nVλ = gcv_tr(Ψ, b̄, x̄₀, 0.1)                         # V(λ) single λ value\nVλ = @_ map(gcv_tr(Ψ, b̄, x̄₀, _), [0.1, 1.0, 10.0]) # V(λ) for array of λ\n\n\n\n\n\n","category":"function"},{"location":"library/","page":"Library","title":"Library","text":"gcv_svd","category":"page"},{"location":"library/#RegularizationTools.gcv_svd","page":"Library","title":"RegularizationTools.gcv_svd","text":"gcv_svd(Ψ::RegularizationProblem, b̄::AbstractVector, λ::AbstractFloat)\n\nCompute the Generalized Cross Validation using the trace term using the SVD  algorithm. Requires that the vector b̄ is in standard form.\n\nExample Usage\n\nusing Underscores\n\nΨ = setupRegularizationProblem(A, 1)            # Setup problem\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)              # Convert to standard form\nVλ = gcv_svd(Ψ, b̄, x̄₀, 0.1)                     # V(λ) single λ value\nVλ = @_ map(gcv_svd(Ψ, b̄, _), [0.1, 1.0, 10.0]) # V(λ) for array of λ\n\n\n\n\n\ngcv_svd(\n    Ψ::RegularizationProblem,\n    b̄::AbstractVector,\n    x̄₀::AbstractVector,\n    λ::AbstractFloat,\n)\n\nCompute the Generalized Cross Validation using the SVD algorithm and intial guess.  Requires that the vectors b̄ and x̄₀ are in standard form.\n\nExample Usage\n\nusing Underscores\n\nΨ = setupRegularizationProblem(A, 1)               # Setup problem\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)                 # Convert to standard form\nVλ = gcv_tr(Ψ, b̄, x̄₀, 0.1)                         # V(λ) single λ value\nVλ = @_ map(gcv_tr(Ψ, b̄, x̄₀, _), [0.1, 1.0, 10.0]) # V(λ) for array of λ\n\n\n\n\n\n","category":"function"},{"location":"library/#L-curve-Functions","page":"Library","title":"L-curve Functions","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"Lcurve_functions","category":"page"},{"location":"library/#RegularizationTools.Lcurve_functions","page":"Library","title":"RegularizationTools.Lcurve_functions","text":"Lcurve_functions(Ψ::RegularizationProblem, b̄::AbstractVector)\n\nCompute the L-curve functions to evaluate the norms L1, L2, and the curvature κ.  Requires that the vectors b̄ is in standard form.\n\nExample Usage\n\nΨ = setupRegularizationProblem(A, 1)\nb̄ = to_standard_form(Ψ, b)\nL1norm, L2norm, κ = Lcurve_functions(Ψ, b̄)\n\nL1norm.([0.1, 1.0, 10.0])    # L1 norm for λ's\nL2norm.([0.1, 1.0, 10.0])    # L2 norm for λ's\nκ.([0.1, 1.0, 10.0])         # L-curve curvature for λ's\n\n\n\n\n\nLcurve_functions(Ψ::RegularizationProblem, b̄::AbstractVector, x̄₀::AbstractVector)\n\nCompute the L-curve functions to evaluate the norms L1, L2, and the curvature κ.  Requires that the vectors b̄ and x̄₀ are in standard form.\n\nExample Usage\n\nΨ = setupRegularizationProblem(A, 1)\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)                 \nL1norm, L2norm, κ = Lcurve_functions(Ψ, b̄, x̄₀)\n\nL1norm.([0.1, 1.0, 10.0])    # L1 norm for λ's\nL2norm.([0.1, 1.0, 10.0])    # L2 norm for λ's\nκ.([0.1, 1.0, 10.0])         # L-curve curvature for λ's\n\n\n\n\n\n","category":"function"},{"location":"library/#Generic-Functions","page":"Library","title":"Generic Functions","text":"","category":"section"},{"location":"library/#designmatrix","page":"Library","title":"designmatrix","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"designmatrix","category":"page"},{"location":"library/#RegularizationTools.designmatrix","page":"Library","title":"RegularizationTools.designmatrix","text":"designmatrix(s::Any, q::Any, f::Function)::AbstractMatrix\n\ns is an array of setpoints\nq is an array of query points\nf is a function that maps the functor Domain to a solution y\n\nThe function to creates an array of domain nodes using the standard basis eᵢ of  the vector space. The setpoint or query point can be any abstract notion of the  input. For examples, a numerical value corresponding to a setting, a string label  ([\"bin A\", \"bin B\", ...), or a list of pixel coordinate [(1,1), (1,2), ...].  The function f must accept a single argument of type Domain and provide a  numerical mapping between input x and output y at query point q. The function designmatrix then returns a design matrix mathbfA such that \n\ny = mathbfAx\n\nwhere x is an array of numerical input values. In the case that [q] = [s], the  shortcut designmatrix(s, f) can be used.\n\n\n\n\n\n","category":"function"},{"location":"library/#forwardmodel","page":"Library","title":"forwardmodel","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"forwardmodel","category":"page"},{"location":"library/#RegularizationTools.forwardmodel","page":"Library","title":"RegularizationTools.forwardmodel","text":"forwardmodel(\n    s::AbstractVector{T1},\n    x::AbstractVector{T2},\n    f::Function,\n)::AbstractArray where {T1<:Any,T2<:Number}\n\nForward model that maps numerical values [x] corresponding to  setpoints [s] to output [y], given the function f. The function f must  accept a single argument of type Domain and provide a  numerical mapping between input x and output y at query point q.\n\nNote that the designmatrix and forward model are related\n\n``` A = designmatrix(s, q, f) y1 = A*x\n\ny2 = forwardmodel(s, x, q, f) y1 == y2\n\n\n\n\n\n","category":"function"},{"location":"library/#High-Level-API","page":"Library","title":"High-Level API","text":"","category":"section"},{"location":"library/#invert","page":"Library","title":"invert","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"invert","category":"page"},{"location":"library/#RegularizationTools.invert","page":"Library","title":"RegularizationTools.invert","text":"function invert(A::Matrix, b::Vector, method::InverseMethod; kwargs...)\n\nHigh-level API function to perform Tikhonov inversion. The function used  the algebraic data type InverseMethod\n\n    @data InverseMethod begin \n        Lₖ(Int)                            # Pure Tikhonov\n        Lₖx₀(Int,Vector)                   # with initial guess\n        LₖB(Int,Vector,Vector)             # with bounds\n        Lₖx₀B(Int,Vector,Vector,Vector)    # with initial guess + bounds\n        LₖDₓ(Int,Float64)                  # with filter \n        Lₖx₀Dₓ(Int,Vector,Float64)         # with initial guess + filter \n        LₖDₓB(Int,Float64,Vector,Vector)   # with filter + bound\n        Lₖx₀DₓB(Int,Vector,Float64,Vector,Vector) # with initial guess + filter + bound\n    end\n\nto define the problem and then dispatches to the correct method. The kwargs... are passed to the solve function For example the standard way to perform second order regularization is \n\nxλ = @> setupRegularizationProblem(A, 2) solve(b) getfield(:x)\n\nthis can alternativel written as \n\ninvert(A, b, Lₖ(2))\n\nwhere Lₖ(2) denotes the second order method. The method nomenclature is Lₖ for regularization order, B for bounded search, x₀ for  initial condition, and Dₓ for the Huckle and Sedlacek (2012) two-step data based  regularization. The method data type takes hyper parameters to initialize the search.  Examples of method initializations are\n\n# Hyper parameters \nk: order, lb: low bound, ub: upper bound, ε: noise level, x₀: initial guess\nk, lb, ub, ε, x₀ = 2, zeros(8), zeros(8) .+ 50.0, 0.02, 0.5*N\n\nxλ = invert(A, b, Lₖ(k); alg = :gcv_tr, λ₁ = 0.1)\nxλ = invert(A, b, Lₖ(k); alg = :gcv_svd, λ₁ = 0.1)\nxλ = invert(A, b, LₖB(k, lb, ub); alg = :L_curve, λ₁ = 0.1)\nxλ = invert(A, b, Lₖx₀(k, x₀); alg = :L_curve, λ₁ = 0.1)\nxλ = invert(A, b, Lₖx₀(k, x₀); alg = :gcv_tr)\nxλ = invert(A, b, Lₖx₀(k, x₀); alg = :gcv_svd)\nxλ = invert(A, b, Lₖx₀B(k, x₀, lb, ub); alg = :gcv_svd)\nxλ = invert(A, b, LₖDₓ(k, ε); alg = :gcv_svd)\nxλ = invert(A, b, LₖDₓB(k, ε, lb, ub); alg = :gcv_svd)\nxλ = invert(A, b, Lₖx₀Dₓ(k, x₀, ε); alg = :gcv_svd)\nxλ = invert(A, b, Lₖx₀DₓB(k, x₀, ε, lb, ub); alg = :gcv_svd)\n\n\n\n\n\n","category":"function"},{"location":"theory/theory/#The-Inverse-Problem","page":"Theory","title":"The Inverse Problem","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Consider the following linear system of equations","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"bf rm bf Arm x=rm y","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"where bf rm bf A is a square design matrix, rm x is a vector of input parameters and rm y is a vector of responses. To estimate unknown inputs from response, the matrix inverse can be used","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm x=rm bf A^-1y","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"However, if a random measurement error epsilon is superimposed on rm y, i.e. b_i=y_i+epsilon_i, the estimate rm hatx from the matrix inverse ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm hatx=rm bf A^-1b","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"becomes dominated by contributions from data error for large systems. ","category":"page"},{"location":"theory/theory/#Example","page":"Theory","title":"Example","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"note: Note\nThe example system is a test problem for regularization methods is taken from MatrixDepot.jl and is the same system used in Hansen (2000).","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Consider the following example system of 100 equations. The matrix rmbfA is 100x100.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot               # hide\nr = mdopen(\"shaw\", 100, false)  # hide\nA, x, y = r.A, r.x, r.b         # hide\n\nA","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The vector rmx of input variables has 100 elements.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot               # hide\nusing Random                    # hide\nr = mdopen(\"shaw\", 100, false)  # hide\nA, x, y = r.A, r.x, r.b         # hide\nRandom.seed!(302)               # hide\n\nx","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Computing rmy and rmb using the pseudo-inverse pinv shows that the error in rmb makes the inversion unusable.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using Cairo               # hide\nusing Fontconfig          # hide\nusing RegularizationTools # hide\nusing MatrixDepot         # hide\nusing Gadfly              # hide\nusing Random              # hide\nusing DataFrames          # hide\nusing LinearAlgebra       # hide\nusing Printf              # hide\nusing Underscores         # hide\nusing Colors #hide\n\ninclude(\"helpers.jl\")     # hide\nr = mdopen(\"shaw\", 100, false)  # hide\nA, x, y = r.A, r.x, r.b         # hide\nRandom.seed!(302)               # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nx = pinv(A) * y\nx̂ = pinv(A) * b\n# hide\nn = length(x) # hide\nd = 1:1:n # hide\ndf1 = DataFrame(x = d, y = y, Color = [\"y\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = b, Color = [\"b\" for i = 1:n]) # hide\ndf = [df1; df2] # hide\np1 = graph(df) # hide\ndf1 = DataFrame(x = d, y = x, Color = [\"x\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = x̂, Color = [\"x̂\" for i = 1:n]) # hide\np2 = graph(df1) # hide\np3 = graph(df2, colors = [\"darkred\"]) # hide\nset_default_plot_size(22cm, 7cm) # hide\nhstack(p1, p2, p3) # hide","category":"page"},{"location":"theory/theory/#Tikhonov-Regularization","page":"Theory","title":"Tikhonov Regularization","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Tikhonov regularization is a means to filter this noise by solving the minimization problem ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"where rm x_lambda is the regularized estimate of rm x, leftlVert cdotrightrVert _2 is the Euclidean norm, rm bf L is the Tikhonov filter matrix, lambda is the regularization parameter, and rm x_0 is a vector of an a priori guess of the solution. The initial guess can be taken to be rm x_0=0 if no a priori information is known. The matrix rm bf A does not need to be square. ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"For lambda=0 the Tikhonov problem reverts to the ordinary least squares solution. If rm bf A is square and lambda=0, the least-squares solution is rm hatx=rm bf A^-1b. For large lambda the solution reverts to the initial guess., i.e. lim_lambdarightarrowinftyrm x_lambda=rm x_0. Therefore, the regularization parameter lambda interpolates between the initial guess and the noisy ordinary least squares solution. The filter matrix rm bf L provides additional smoothness constraints on the solution. The simplest form is to use the identity matrix, rm bf L=rm bf I.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The formal solution to the Tikhonov problem is given by","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm x_lambda=left(rm bf A^Trm bf A+lambda^2rm bf L^Trm bf Lright)^-1left(rm bf A^Trm b+lambda^2rm bf L^Trm bf Lrm x_0right)","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The equation is readily derived by writing f=leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2, take fracdfdrm rm x=0, and solve for rm x. Use http://www.matrixcalculus.org/ to validate symbolic matrix derivatives.","category":"page"},{"location":"theory/theory/#Example-2","page":"Theory","title":"Example","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Here is a simple regularized inversion for the same system using rm bf L=rm bf I and rm x_0=0. The regularized solution is ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm x_lambda=left(rm bf A^Trm bf A+lambda^2rm bf Iright)^-1rm bf A^Trm b","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The regularized inverse can be trivially computed assuming a value for lambda = 011.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using RegularizationTools # hide\nusing MatrixDepot # hide\nusing Gadfly # hide\nusing Random # hide\nusing DataFrames # hide\nusing Colors # hide\nusing LinearAlgebra # hide\nusing Underscores # hide\nusing Printf #  hide\n\ninclude(\"helpers.jl\") # hide\nr = mdopen(\"shaw\", 100, false) # hide\nA, x, y = r.A, r.x, r.b # hide\nRandom.seed!(716) # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nIₙ = Matrix{Float64}(I, 100, 100)\nλ = 0.11\nxλ = inv(A'A + λ^2.0 * Iₙ) * A' * b\n\nn = length(x) # hide\nd = 1:1:n # hide\ndf1 = DataFrame(x = d, y = y, Color = [\"y\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = b, Color = [\"b\" for i = 1:n]) # hide\ndf = [df1; df2] # hide\np1 = graph(df) # hide\n# hide\ndf1 = DataFrame(x = d, y = x, Color = [\"x\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = xλ, Color = [\"xλ\" for i = 1:n]) # hide\ndf = [df1; df2] # hide\np2 = graph(df) # hide\nset_default_plot_size(15cm, 6cm) # hide\nhstack(p1, p2) # hide","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The solution is not perfect, but it is free of random error and a reasonable approximation of the true rmx.","category":"page"},{"location":"theory/theory/#Optimal-Regularization-Parameter","page":"Theory","title":"Optimal Regularization Parameter","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The choice of the optimal regularization parameter is not obvious. If we pick lambda too small, the solution is dominated by noise. If we pick lambda too large the solution will not approximate the correct solution.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using RegularizationTools # hide\nusing MatrixDepot # hide\nusing Gadfly # hide\nusing Random # hide\nusing DataFrames # hide\nusing Colors # hide\nusing LinearAlgebra # hide\nusing Underscores #hide\nusing Printf #  hide\n\ninclude(\"helpers.jl\") # hide\nr = mdopen(\"shaw\", 100, false) # hide\nA, x, y = r.A, r.x, r.b # hide\nRandom.seed!(716) # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nIₙ = Matrix{Float64}(I, 100, 100)\nf(λ) = inv(A'A + λ^2.0 * Iₙ) * A' * b\nxλ1 = f(0.001)\nxλ2 = f(0.1) \nxλ3 = f(10.0)\n\nn = length(x) # hide\nd = 1:1:n # hide\ndf1 = DataFrame(x = d, y = y, Color = [\"y\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = b, Color = [\"b\" for i = 1:n]) # hide\ndf = [df1; df2] # hide\np1 = graph(df) # hide\n# hide\ndf1 = DataFrame(x = d, y = x, Color = [\"x\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = xλ1, Color = [\"xλ1\" for i = 1:n]) # hide\ndf3 = DataFrame(x = d, y = xλ2, Color = [\"xλ2\" for i = 1:n]) # hide\ndf4 = DataFrame(x = d, y = xλ3, Color = [\"xλ3\" for i = 1:n]) # hide\ndf = [df1; df2; df3; df4] # hide\np2 = graph(df; colors = [\"black\", \"steelblue3\", \"darkred\", \"darkgoldenrod\"]) # hide\nset_default_plot_size(15cm, 6cm) # hide\nhstack(p1, p2) # hide","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"lambda = 01 provides an acceptable solution. lambda = 0001 is noisy (under-regularized) and lambda = 100 is incorrect (over-regularized). There are several objective methods to find the optimal regularization parameter. The general procedure to identify the optimal lambda is to compute rm x_lambda for a range of regularization parameters over the interval [lambda_1, lambda_2] and then apply some evaluation criterion that objectively evaluates the quality of the solution. This package implements two of these, the L-curve method and generalized cross validation.","category":"page"},{"location":"theory/theory/#L-Curve-Method","page":"Theory","title":"L-Curve Method","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The L-curve method evaluates the by balancing the size of the residual norm L_1=leftlVert bf rm bf Arm x_lambda-rm brightrVert _2 and the size of the solution norm L_2=leftlVert rm bf L(rm x_lambda-rm x_0)rightrVert _2 for rm x_lambdainlambda_1lambda_2. The L-curve consists of a plot of log L_1 vs. log L_1. The following example illustrates the L-curve without specifying an a priori input.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot # hide\nusing Gadfly  # hide\nusing Random # hide\nusing DataFrames # hide\nusing Colors # hide\nusing LinearAlgebra # hide\nusing Underscores # hide\nusing NumericIO # hide\nusing Printf # hide\n\ninclude(\"helpers.jl\") # hide\nr = mdopen(\"shaw\", 100, false) # hide\nA, x, y = r.A, r.x, r.b # hide\nRandom.seed!(716) # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nIₙ = Matrix{Float64}(I, 100, 100)\nf(λ) = inv(A'A + λ^2.0 * Iₙ) * A' * b\nL1(λ) = norm(A * f(λ) - b)\nL2(λ) = norm(Iₙ * f(λ))\nλs = exp10.(range(log10(1e-5), stop = log10(10), length = 100))\nresidual, solution = L1.(λs), L2.(λs)\n\nset_default_plot_size(5inch, 3.5inch) # hide\ngraph1(residual, solution) # hide","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The optimal lambda_opt is the corner of the L-curve. In this example this is lambda_opt approx 01, which yielded the acceptable solution earlier. Finding the corner of the L-curve can be automated by performing an gradient descent search to find the mimum value of the curvature of the L-curve (Hansen, 2000). The implementation is discussed in the L-Curve Algorithm section.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The solve function in RegularizationTools can be used to find λopt through the L-curve algorithm, searching over the predefined interval [lambda_1, lambda_2].","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot # hide\nusing Lazy   # hide\nusing Random # hide\nusing RegularizationTools\n\nr = mdopen(\"shaw\", 100, false) #hide\nA, x, y = r.A, r.x, r.b   # hide\nRandom.seed!(716)  # hide\ny = A * x\nb = y + 0.1y .* randn(100)\nsolution = @> setupRegularizationProblem(A, 0) solve(b, alg = :L_curve, λ₁ = 0.01, λ₂ = 1.0)\nλopt = solution.λ","category":"page"},{"location":"theory/theory/#Generalized-Cross-Validation","page":"Theory","title":"Generalized Cross Validation","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"If the general form of the problem ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"has the smoothing matrix rm bf L=rm bf I, the problem is considered to be in standard form. The general-form problem can be transformed into standard form (see Transformation to Standard Form for algorithm). If the problem is in standrad form, and if rm x_0=0, the GCV estimate of lambda is (Golub et al., 1979):","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"V(lambda)=fracnleftlVert (bf rm bf bf I-bf A_lambda)rm brightrVert _2^2tr(rm bf I-rm bf A_lambda)^2","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"where bf A_lambda=rm rm bf Aleft(bf AA^T-lambda^2rm bf Iright)^-1bf A^T is the influence matrix, tr is the matrix trace, n is the size of rmb. Note that rm x_lambda=left(rm bf A^Trm bf A+lambda^2rm bf Iright)^-1rm bf A^Trm b. Therefore rm rm rm bf Ax_lambda=rm bf A_lambdab and leftlVert (bf rm bf bf I-bf A_lambda)rm brightrVert _2=leftlVert bf rm bf Arm x_lambda-rm brightrVert _2. The optimal lambda_opt coincides with the global minimum of V(lambda). ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The following example evaluates V(lambda) over a range of lambda. ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot #hide\nusing Gadfly #hide\nusing Random #hide\nusing Colors #hide\nusing DataFrames #hide\nusing Printf#hide\nusing Lazy#hide\nusing Underscores #hide\nusing LinearAlgebra #hide\ninclude(\"helpers.jl\") #hide\nr = mdopen(\"shaw\", 100, false) #hide\nA, x, y = r.A, r.x, r.b #hide\nRandom.seed!(716) #hide\ny = A * x\nb = y + 0.1y .* randn(100)\nIₙ = Matrix{Float64}(I, 100, 100)\nAλ(λ) = A*inv(A'A + λ^2.0*Iₙ)*A'\ngcv(λ) = 100*norm((Iₙ - Aλ(λ))*b)^2.0/tr(Iₙ - Aλ(λ))^2.0\nλs = exp10.(range(log10(1e-3), stop = log10(1), length = 100))\nV = map(gcv, λs)\n#hide\nset_default_plot_size(5inch, 3.5inch) # hide\ngraph2(λs, V) #hide","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The GCV curve has a steep part for large lambda and a shallow part for small lambda. The minimum occurs near lambda = 01.  The solve function in RegularizationTools can be used to find λopt through the GCV approach, searching over the predefined interval [lambda_1, lambda_2].","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot # hide\nusing Lazy   # hide\nusing Random # hide\nusing RegularizationTools\n\nr = mdopen(\"shaw\", 100, false) #hide\nA, x, y = r.A, r.x, r.b   # hide\nRandom.seed!(716)  # hide\ny = A * x\nb = y + 0.1y .* randn(100)\nsolution = @> setupRegularizationProblem(A, 0) solve(b, alg = :gcv_svd, λ₁ = 0.01, λ₂ = 1.0)\nλopt = solution.λ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Note that the objective λopt from the L-curve and GCV criterion are nearly identical. ","category":"page"},{"location":"theory/theory/#Transformation-to-Standard-Form","page":"Theory","title":"Transformation to Standard Form","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The general from couched in terms of rmbfA rm b rm x rm x_0 and rmbfL","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"is transformed to standard form couched in terms of rm bf bar A rm bar b rm bar x rm barx_0 and rmbfI","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm rm x_lambda=argminleft leftlVert bf rm bf bar Arm x-rm bar brightrVert _2^2+lambda^2leftlVert rm bf I(rm bar x-rm bar x_0)rightrVert _2^2right","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"as introduced by Eldén (1977) using notation from Hansen (1998, Chapter 2.3.1). The algorithm computes the explicit transformation using two QR factorizations. The matrices needed for the explicit conversion depend on rm bf A and rm bf L and are computed and cached in setupRegularizationProblem. ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The λopt search is performed in in standard form, and the solution is computed in standard form. Then the resulting solution is transformed back to the general form using the same matrices.","category":"page"},{"location":"theory/theory/#Solving-the-Standard-Equations","page":"Theory","title":"Solving the Standard Equations","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The solution rm bar x_lambda for the transformed standard equation is ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm bar x_lambda=left(rm bf bar A^Trm bf bar A+lambda^2rm bf Iright)^-1 left( rm bf bar A^Trm bar b + λ^2 rm bar x_0 right)","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm bar x_lambda is found using the Cholesky decomposition. It is the alorithm used in the MultivariateStats package and generally faster than the QR approach (Lira et al., 2016). In case the Cholesky factorization fails, the default julia equation solver is used.","category":"page"},{"location":"theory/theory/#L-Curve-Algorithm","page":"Theory","title":"L-Curve Algorithm","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The curvature kappa(lambda) of the L-curve is computed using Eq. (18) in Hansen (2000). The expression requires calculation of the solution and residual norms, as well as the first derivative of the solution norm. The derivative is calculated using finite differences from the Calculus package The corner of the L-curve occurs when the curvature maximizes. Finally,  λ_opt is found by minimizing the -kappa(lambda) function (see L-Curve Method) on a bounded interval using Brent's method, as implemented in the Optim package.","category":"page"},{"location":"theory/theory/#Generalized-Cross-Validation-Algorithm","page":"Theory","title":"Generalized Cross Validation Algorithm","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"V(lambda)=fracnleftlVert (bf rm bf bf I-bf bar A_lambda)rm bar brightrVert _2^2tr(rm bf I-rm bf bar A_lambda)^2","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The slowest part in the GCV calculation is evaluation of the trace.  The GCV estimate is computed either using the single value decomposition (SVD) algorithm (Golub et al., 1979) (gcv_svd) or the explicit calculation using the trace term (gcv_tr). The SVD of the design matrix in standard form rm bfbar A is calculated and cached in setupRegularizationProblem. When an initial guess is included, the denominator is computed using the SVD estimate and the numerator is computed via leftlVert bf rm bf bar Arm bar x_lambda-rm bar brightrVert _2^2 and rm bar x_lambda is obtained using the Cholesky decomposition algorithm solving for the Tikhonov solution in standard form with an initial guess. Note that this is an approximation because the trace term in the denominator does not account for the intial guess. Comparison with the L-curve method suggest that this approximation does not affect the quality of the regularized solution. Finally, λ_opt is found by minimizing V(lambda) on a bounded interval using Brent's method, as implemented in the Optim package.","category":"page"},{"location":"manual/#Manual","page":"Manual","title":"Manual","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"A theoretical background of Tikhonov regularization is provided in the The Inverse Problem section. ","category":"page"},{"location":"manual/#Solving-a-Problem","page":"Manual","title":"Solving a Problem","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"A problem consists of a design matrix rm bf A and a vector rm b such that","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm b = bf rm bf Arm x + epsilon","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where epsilon is noise and the objective is to reconstruct the original parameters rm x. The solution to the problem is to minimize","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where rm x_lambda is the regularized estimate of rm x, leftlVert cdotrightrVert _2 is the Euclidean norm, rm bf L is the Tikhonov filter matrix, lambda is the regularization parameter, and rm x_0 is a vector of an a priori guess of the solution. The initial guess can be taken to be rm x_0=0 if no a priori information is known. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The basic steps to solve the problem are ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Ψ = setupRegularizationProblem(A, 2)   # Setup the problem \nsolution = solve(Ψ, b)                 # Compute the solution \nxλ = solution.x                        # Extract the x","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The solve function finds Optimal Regularization Parameter, by default using Generalized Cross Validation. It applies the optimal lambda value to compute rm x_lambda. The solution is of type RegularizatedSolution, which contains the optimal solution (solution.x), the optimal lambda (solution.λ) and output from the otimization routine (solution.solution).","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"An convenient way to find x is using Lazy pipes:","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"xλ = @> setupRegularizationProblem(A, 2) solve(b) getfield(:x)","category":"page"},{"location":"manual/#Specifying-the-Order","page":"Manual","title":"Specifying the Order","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Common choices for the bfrmL matrix are finite difference approximations of a derivative. There are termed zeroth, first, and second order inversion matrices. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm bf L_0=left(beginarrayccccc\n1        0\n  1\n    ddots\n      1\n0        1\nendarrayright)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm bf L_1=left(beginarraycccccc\n1  -1        0\n  1  -1\n    ddots  ddots\n      1  -1\n0        1  -1\nendarrayright)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm bf L_2=left(beginarrayccccccc\n-1  2  -1        0\n  -1  2  -1\n    ddots  ddots  ddots\n      -1  2  -1\n0        -1  2  -1\nendarrayright)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"You can specify which of these matrices to use in ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"setupRegularizationProblem(A::AbstractMatrix, order::Int)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where order = 0, 1, 2 corresponds to bfrmL_0, bfrmL_1, and  bfrmL_2","category":"page"},{"location":"manual/#Example-:-[Phillips-Problem](https://matrixdepotjl.readthedocs.io/en/latest/regu.html#term-phillips)","page":"Manual","title":"Example : Phillips Problem","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Example with 100 point discretization and zero initial guess.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy\nusing Random # hide\n\nr = mdopen(\"phillips\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(850) # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nxλ = @> setupRegularizationProblem(A, 2) solve(b) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot(y, b, x, xλ, 0.0x) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nThe random perturbation b = y + 0.1y .* randn(100)  in each of the examples uses a fixed random seed to ensure reproducibility. The random seed and plot commands are hidden for clarity. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nThe example system is a test problem for regularization methods is taken from MatrixDepot.jl and is the same system used in Hansen (2000).","category":"page"},{"location":"manual/#Example-2:-[Shaw-Problem](https://matrixdepotjl.readthedocs.io/en/latest/regu.html#term-shaw)","page":"Manual","title":"Example 2: Shaw Problem","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Zeroth order example with 500 point discretization and moderate initial guess.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random\n\nr = mdopen(\"shaw\", 500, false)\nA, x = r.A, r.x\nRandom.seed!(850) #hide \n\ny = A * x\nb = y + 0.1y .* randn(500)\nx₀ = 0.6x\n\nxλ = @> setupRegularizationProblem(A, 0) solve(b, x₀) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot(y, b, x, xλ, x₀)# hide","category":"page"},{"location":"manual/#Using-a-Custom-L-Matrix","page":"Manual","title":"Using a Custom L Matrix","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"You can specify custom rm bf L matrices when setting up problems.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"setupRegularizationProblem(A::AbstractMatrix, L::AbstractMatrix)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"For example, Huckle and Sedlacek (2012) propose a two-step data based regularization","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm bf L = rm bf L_k rm bf D_hatx^-1 ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where rm bf L_k is one of the finite difference approximations of a derivative,  rm bf D_hatx=diag(hatx_1ldotshatx_n), hatx is the reconstruction of x using rm bf L_k, and (rm bf D_hatx)_ii=epsilonforallhatx_iepsilon, with epsilon  1. ","category":"page"},{"location":"manual/#Example-3:-[Heat-Problem-](https://matrixdepotjl.readthedocs.io/en/latest/regu.html#term-heat)","page":"Manual","title":"Example 3: Heat Problem ","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"This examples illustrates how to implement the Huckle and Sedlacek (2012) matrix. Note that Γ(A, 2) returns the Tikhonov Matrix of order 2. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random, LinearAlgebra\n\nr = mdopen(\"heat\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(150) #hide\n\ny = A * x\nb = y + 0.05y .* randn(100)\nx₀ = zeros(length(b))\n\nL₂ = Γ(A,2)                  \nxλ1 = @> setupRegularizationProblem(A, L₂) solve(b) getfield(:x)\nx̂ = deepcopy(abs.(xλ1))\nx̂[abs.(x̂) .< 0.1] .= 0.1\nL = L₂*Diagonal(x̂)^(-1)\nxλ2 = @> setupRegularizationProblem(A, L) solve(b) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot1(y, b, x, xλ1, xλ2) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The solution xλ2 is improved over the regular L₂ solution. ","category":"page"},{"location":"manual/#Adding-Boundary-Constraints","page":"Manual","title":"Adding Boundary Constraints","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"To add boundary constraints (e.g. enforce that all solutions are positive), the following procedure is implemented. Compute the algebraic solution without constraints, truncate the solution at the upper and lower bounds, and use the result as initial condition for solving the minimization problem with a least squares numerical solver LeastSquaresOptim. The regularization parameter λ obtained from the algebraic solution is used for a single pass optimization. See solve for a complete list of methods.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random, LinearAlgebra\n\nr = mdopen(\"heat\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(150) #hide\n\ny = A * x\nb = y + 0.05y .* randn(100)\nx₀ = zeros(length(b))\n\nL₂ = Γ(A,2)                  \nxλ1 = @> setupRegularizationProblem(A, L₂) solve(b) getfield(:x)\nx̂ = deepcopy(abs.(xλ1))\nx̂[abs.(x̂) .< 0.1] .= 0.1\nψ = @>> L₂*Diagonal(x̂)^(-1) setupRegularizationProblem(A) \nlower = zeros(100)\nupper = ones(100)\nxλ2 = @> solve(ψ, b, lower, upper) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot1(y, b, x, xλ1, xλ2) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"This is the same example as above, but imposing a lower and upper bound on the solution. Note that this solve method computes the algebraic solution using solve(Ψ, b; kwargs...) method to compute the starting point for the least square minimization. You can also call","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"xλ2 = @> solve(ψ, b, x₀, lower, upper) getfield(:x)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"to bound the least-square solver by the output from the solve(Ψ, b, x₀; kwargs...) method.","category":"page"},{"location":"manual/#Customizing-the-Search-Algorithm","page":"Manual","title":"Customizing the Search Algorithm","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"The solve function searches for the optimum regularization parameter lambda between lambda_1 lambda_2. The default search range is [0.001, 1000.0] and the interval range can be modified through keyword parameters. The optimality criterion is either the minimum of the Generalized Cross Validation function, or the the maximum curvature of the L-curve (see L-Curve Algorithm). The algorithm can be specified through the alg keyword. Valid algorithms are :L_curve, :gcv_svd, and :gcv_tr (see solve).","category":"page"},{"location":"manual/#Example","page":"Manual","title":"Example","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random\n\nr = mdopen(\"shaw\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(100) #hide\n\ny = A  * x\nb = y + 0.1y .* randn(100)\n\nxλ1 = @> setupRegularizationProblem(A,1) solve(b, alg=:L_curve, λ₁=0.1, λ₂=10.0) getfield(:x)\nxλ2 = @> setupRegularizationProblem(A,1) solve(b, alg=:gcv_svd) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nx₀ = 0.0*x # hide\nstandard_plot1(y, b, x, xλ1, xλ2) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Note that the output from the L-curve and GCV algorithm are nearly identical. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nThe L-curve algorithm is more sensitive to the bounds and slower than the gcv_svd algorithm. There may, however, be cases where the L-curve approach is preferable. ","category":"page"},{"location":"manual/#Extracting-the-Validation-Function","page":"Manual","title":"Extracting the Validation Function","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"The solution is obtained by first transforming the problem to standard form (see Transformation to Standard Form). The following example can be used to extract the GCV function.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random, Underscores, Printf\n\nr = mdopen(\"shaw\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(716) #hide\n\ny = A  * x\nb = y + 0.1y .* randn(100)\n\nΨ = setupRegularizationProblem(A,1)\nλopt = @> solve(Ψ, b, alg=:gcv_svd) getfield(:λ)\nb̄ = to_standard_form(Ψ, b) \nλs = exp10.(range(log10(1e-1), stop = log10(10), length = 100))\nVλ = @_ map(gcv_svd(Ψ, b̄, _), λs) \ninclude(\"theory/helpers.jl\") # hide\ngraph3(λs, Vλ) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The calculated λopt from ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"λopt = @> solve(Ψ, b, alg=:gcv_svd) getfield(:λ)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"is 1.1 and corresponds to the minimum of the GCV curve. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Alternatively, the L-curve is retrieved through the L-curve Functions","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random, Underscores, Printf\n\nr = mdopen(\"shaw\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(716) #hide\n\ny = A  * x\nb = y + 0.1y .* randn(100)\n\nΨ = setupRegularizationProblem(A,1)\nλopt = @> solve(Ψ, b, alg=:L_curve, λ₂ = 10.0) getfield(:λ)\nb̄ = to_standard_form(Ψ, b) \nλs = exp10.(range(log10(1e-3), stop = log10(100), length = 200))\nL1norm, L2norm, κ = Lcurve_functions(Ψ, b̄)\nL1, L2 = L1norm.(λs), L2norm.(λs)    \ninclude(\"theory/helpers.jl\") # hide\ngraph4(L1, L2) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The calculated λopt from ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"λopt = @> solve(Ψ, b, alg=:L_curve, λ₂ = 10.0) getfield(:λ)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"is 0.9 and corresponds to the corner of the L-curve.","category":"page"},{"location":"manual/#The-Invert-Function","page":"Manual","title":"The Invert Function","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"The code to perform second order regularization us","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"xλ = @> setupRegularizationProblem(A, 2) solve(b) getfield(:x)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"this can become a bit verbose, especially with more elaborate inversion methods. The invert function provides a high-level API. For example the code above can be written as","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"xλ = invert(A, b, Lₖ(2))","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"here Lₖ(2) denotes the second order inversion and \"2\" is a hyperparameter that specifies the order. Several methods are predefined. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The method nomenclature is ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Lₖ for regularization order (see Specifying the Order; the hyperparameter is the order k)\nB for bounded search (see Adding Boundary Constraints; the hyperparameters are the lower and upper bound, lb and ub)\nx₀ for adding an intial guess\nDₓ for the Huckle and Sedlacek (2012) two-step data based regularization (see Using a Custom L Matrix; the hyperparameter is the noise level ε). ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The InverseMethod data type takes the hyper parameters as arguments. The invert function passes all keyword arguments through to the solve function. Examples of method initializations are","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"# Hyper parameters \nk: order, lb: low bound, ub: upper bound, ε: noise level, x₀: initial guess\nk, lb, ub, ε, x₀ = 2, zeros(8), zeros(8) .+ 50.0, 0.02, 0.5*N\n\nxλ = invert(A, b, Lₖ(k); alg = :gcv_tr, λ₁ = 0.1)\nxλ = invert(A, b, Lₖ(k); alg = :gcv_svd, λ₁ = 0.1)\nxλ = invert(A, b, LₖB(k, lb, ub); alg = :L_curve, λ₁ = 0.1)\nxλ = invert(A, b, Lₖx₀(k, x₀); alg = :L_curve, λ₁ = 0.1)\nxλ = invert(A, b, Lₖx₀(k, x₀); alg = :gcv_tr)\nxλ = invert(A, b, Lₖx₀(k, x₀); alg = :gcv_svd)\nxλ = invert(A, b, Lₖx₀B(k, x₀, lb, ub); alg = :gcv_svd)\nxλ = invert(A, b, LₖDₓ(k, ε); alg = :gcv_svd)\nxλ = invert(A, b, LₖDₓB(k, ε, lb, ub); alg = :gcv_svd)\nxλ = invert(A, b, Lₖx₀Dₓ(k, x₀, ε); alg = :gcv_svd)\nxλ = invert(A, b, Lₖx₀DₓB(k, x₀, ε, lb, ub); alg = :gcv_svd)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The formal definition of the InverseMethod data structure is","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"    @data InverseMethod begin \n        Lₖ(Int)                              # Pure Tikhonov\n        Lₖx₀(Int, Vector)                    # with initial guess\n        LₖB(Int, Vector, Vector)             # with bounds\n        Lₖx₀B(Int, Vector, Vector, Vector)   # with initial guess + bounds\n        LₖDₓ(Int, Float64)                   # with filter \n        Lₖx₀Dₓ(Int, Vector, Float64)         # with initial guess + filter \n        LₖDₓB(Int, Float64, Vector, Vector)  # with filter + bound\n        Lₖx₀DₓB(Int, Vector, Float64, Vector, Vector) # with initial guess + filter + bound\n    end","category":"page"},{"location":"manual/#Creating-a-Design-Matrix","page":"Manual","title":"Creating a Design Matrix","text":"","category":"section"},{"location":"manual/#Standard-Approach","page":"Manual","title":"Standard Approach","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"This package provides an abstract generic interface to create a design matrix from a forward model of a linear process. To understand this functionality, first consider the standard approach to find the design matrix by discretization of the Fredholm integral equation.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Consider a one-dimensional Fredholm integral equation of the first kind on a finite interval:","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"int_a^bK(qs)f(s)ds=g(q)sinabmathrmandqincd","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where K(qs) is the kernel. The inverse problem is to find f(s) for a given K(qs) and g(q).","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The integral equation can be cast as system of linear equations such that","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"mathbfAmathrmx=b","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where mathrmx=x_1dotsx_n=f(s_i) is a discrete vector representing f, mathrmb=b_1dotsb_n=g(q_j) is a vector representing g and mathrmmathbfA is the ntimes n design matrix.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Using the quadrature method (Hansen, 2008), the integral is approximated by a weighted sum such that","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"int_a^bK(q_is)f(s)ds=g(q_i)approxsum_j=1^nwK(q_is_j)f(s_j)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where w=fracb-an, and s_j=(j-frac12)w. The elements comprising the design matrix mathrmmathbfA are a_ij=wK(q_is_j).","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"This simple kernel (Baart, 1982) serves as an illustrative example","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"K(qs)=exp(qcos(s))","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"f(s)=sin(s)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"g(q)=frac2sin ss","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"sin0pimathrmandqin0pi2","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The following imperative code discretizes the problem with n=12 points:","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"a, b = 0.0, π\nn, m = 12,12\nc, d = 0.0, π/2\nA = zeros(n,m)\nw = (b-a)/n\nbaart(x,y) = exp(x*cos(y))\nq = range(c, stop = d, length = n)\ns = [(j-0.5)*(b-a)/n for j = 1:m]\n\nfor i = 1:n\n    for j = 1:m\n        A[i,j] = w*baart(q[i], (j-0.5)*w)\n    end\nend","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"In this example, the functions f(s) and g(q) are known. Evaluating mathrmb=mathrmmathbfAx with mathrmmathrmx=sin(s) approximately yields 2sin(q)q. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"x = sin.(s)\nb1 = A*x \nb2 = 2.0.*sinh.(q)./q","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Here, b1 and b2 are close. However, as noted by Hansen (2008), the product mathbfAmathrmx is, in general, different from g(q) due to discretization errors.","category":"page"},{"location":"manual/#Alternative-Approach","page":"Manual","title":"Alternative Approach","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Discretization of the kernel may be less straight forward for more complex kernels that describe physical processes. Furthermore, in physical processes or engineering systems, the mapping of variables isn't immediately clear. This is especially true when thinking about kernel functions of processes that have not yet been modeled in the literature.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Imagine the following problem studying a system of interest with some instrument. The instrument has a dial and intends to measure a physical property x that is expressible as a numeric value. The dial can be any abstract notion such as \"channel number\", \"input voltage\", or even a combination of parameters such as \"channel number 1 at high gain\", \"channel number 1 at low gain\", and so forth. At each dial setting the instrument passes the physical property through a filter that produces an observable reading b. The domain of the problem consists of a list of valid set points s_i, with a corresponding list of physical properties x_i.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"(Image: image)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"We can then defined a node element of the domain as a product data type","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"struct Domain{T1<:Any,T2<:Number,T3<:Any}\n    s::AbstractVector{T1}\n    x::AbstractVector{T2}\n    q::T3\nend","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"A single node in the domain is defined as","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"node = Domain([s], [x], q)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where q is an abstract query value of the instrument. The forward problem is to defined a function that maps a node of the domain to the observable b, i.e.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"b = f(node) ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Here b corresponds to a numerical value displayed by the detector at setting q.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The higher order function designmatrix","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"function designmatrix(s::Any, q::Any, f::Function)::AbstractMatrix\n    n = length(s)\n    x(i) = @_ map(_ == i ? 1.0 : 0.0, 1:n)\n    nodes = [Domain(s, x(i), q[j]) for i ∈ 1:n, j ∈ 1:length(q)]\n    return @> map(f, nodes) transpose copy\nend","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"maps the list of setpoints s, query points q to the design matrix A for any function f(node) that can operate and the defined setpoints.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Then the mathrmb=mathrmmathbfAx is the linear transformation from a list of physical properties x_i at query points q_i.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Below are three examples on how the function designmatrix works. Complete examples are included in the examples folder.","category":"page"},{"location":"manual/#Example-1","page":"Manual","title":"Example 1","text":"","category":"section"},{"location":"manual/#Problem-Setup","page":"Manual","title":"Problem Setup","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Consider a black and white camera taking a picture in a turbulent atmosphere. The camera has 200x200 pixel resolution. The input values to the camera correspond to the photon count hitting the detector. The output corresponds to a gray scale value between 0 and 1. Let us ignore other effects such as lens distortions, quantum efficiency of the detector, wavelength dependency, or electronic noise. However, the turbulent atmosphere will blur the image by randomly redirecting photons. The Gaussian blur kernel function is","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"K(xy)=frac12pisigmaexpleft(-fracx^2+y^22sigma^2right)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where x, y are the the distances from the pixel coordinate in the horizontal and vertical axis, and sigma is the standard deviation of the Gaussian distribution. The value of K(xy) is zero for pixels further than xc and yc.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Each setpoint is s_i is a tuple of physical coordinates (x,y). The domain for setpoints is a vector of tuples covering all pixels. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using Lazy\n\ns = @> [(i,j) for i = 0:20, j = 0:20] reshape(:)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The domain of query points q_i are pixel coordinates. For this problem it is sensible that s=q, but this need not be the case. The function f(node)rightarrow b is obtained via","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"function get_domainfunction(σ, c)\n    blur(x,y) = 1.0/(2.0*π*σ^2.0)*exp(-(x^2.0 + y^2.0)/(2.0*σ^2.0))\n    \n    function f(node::Domain)\n        s, x, q = node.s, node.x, node.q\n        x₁, y₁ = q[1], q[2]\n\n        y = mapfoldl(+, 1:length(x)) do i\n            x₀, y₀ = s[i][1], s[i][2]   \n            Δx, Δy = abs(x₁-x₀), abs(y₁-y₀)\n            tmp = (Δx < c) && (Δy < c) ? blur(Δx,Δy) * x[i] : 0.0\n        end\n    end\nend\n\nf = get_domainfunction(2.0, 4)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Note that the function f is specialized for a particular σ and c value. The design matrix is then obtained via","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"s = @> [(i,j) for i = 0:20, j = 0:20] reshape(:)\nq = s\nA = designmatrix(s, q, f) ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Finally, the blurred image is computed via","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using Lazy\n\nimg = @> rand(21,21) reshape(:)\nb1 = A*img","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The design matrix is n^2 times n^2, here 441 elements. The image is flattened to a 1D array and the blurred image is obtained via matrix multiplication. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Alternatively, this package provides the higher order function forwardmodel, which performs the equivalent calculation using the specialized function x. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"function forwardmodel(\n    s::AbstractVector{T1},\n    x::AbstractVector{T2},\n    q::AbstractVector{T3},\n    f::Function,\n)::AbstractArray where {T1<:Any,T2<:Number,T3<:Any}\n    return @>> (@_ map(Domain(s, x, _), q)) map(f)\nend","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"for example","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"b1 = A*img\nb2 = forwardmodel(s, img, q, f) ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"and b1 and b2 are approximately equal.","category":"page"},{"location":"manual/#Notes","page":"Manual","title":"Notes","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"The returned function f is specialized for a specific value sigma. In general, the forward model may depend on a substantial number of hyper parameters.\nThe setpoints s and query points q are interpreted by the function f. They therefore can by of any valid type.\nThe types if x and b need not be the same, although both must be a subtype of Number.\nUnsurprisingly the matrix operation is much faster than the forwardmodel.\nThe matrix A can be used in principle to blur and deblur images. You can try it with tiny images. Unfortunately, even a small 200x200 pixel image produces a 40000x40000 matrix, which is too large for the algorithms implemented in this package.\nThe matrix A produced by the function designmatrix is by default a dense matrix. In practice the blur matrix of a well-ordered image is a Toeplitz matrix that can be stored in sparse format. The generic function cannot account for this.\nThe computation time using designmatrix is slower (or much slower for large images) than the explicit discretization of the kernel function.\nFinally, the advantage of the designmatrix approach is to allow for easier conceptualization of the problem using type signatures and a declarative programming interface. This allows for factoring out some of the mathematical details of the discretization problem. It also immediately allows to apply the algorithm to non-square images nor does it rely on a particular sorting of the pixels to construct the matrix.","category":"page"},{"location":"manual/#Example-2","page":"Manual","title":"Example 2","text":"","category":"section"},{"location":"manual/#Problem-Setup-2","page":"Manual","title":"Problem Setup","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Consider a population of particles suspended in air. The particle size distribution is represented in 8 size channels between 100 and 1000 nm. The populations light scattering and light absorption properties are measured at 3 wavelength. The properties are the convolution between the particle size distribution and the optical properties determined by Mie theory.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"beta=int_D_p1^D_p2QfracdNdD_pdD_p","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where Q is either the scattering or absorption cross section of the particle that can be computed from Mie theory, represented through a function Mie(ref,Dp,λ), which is a function of the complex refractive index of the particle (ref), particle diameter (D_p), and wavelength (lambda). Integration is performed over the size distribution fracdNdD_p and the interval D_p1D_p2. In this example, the domain s comprises the 8 particle diameters","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"include(\"size_functions.jl\") # hide \nDp, N = lognormal([[100.0, 500.0, 1.4]], d1 = 100.0, d2 = 1000.0, bins = 8) # hide\n\ns = zip([\"Dp$i\" for i = 1:length(Dp)],Dp) |> collect","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The setpoints s are an array of tuples with labeled diameter and midpoint diameters in units of [nm]. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The query points are the scattering and absorption cross sections measured at three different wavelength","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"λs = [300e-9, 500e-9, 900e-9] # hide\nq1 = zip([\"βs$i\" for i in Int.(round.(λs*1e9, digits=0))], λs) |> collect #hide\nq2 = zip([\"βa$i\" for i in Int.(round.(λs*1e9, digits=0))], λs) |> collect # hide\n\nq = [q1; q2] ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Thus the query points q are an array of tuples with labels and wavelengths in units of [m]. Note that beta_s denotes scattering and beta_a the absorption cross section of the aerosol.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The design matrix is obtained via","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"function get_domainfunction(ref)   \n    function f(node::Domain)\n        s, x, q = node.s, node.x, node.q\n        Dp, λ = (@_ map(_[2], s)), q[2]\n       \n        Q = @match q[1][1:3] begin \n            \"βs\" => @_ map(Qsca(π*Dp[_]*1e-9/λ, ref), 1:length(Dp))\n            \"βa\" => @_ map(Qabs(π*Dp[_]*1e-9/λ, ref), 1:length(Dp))\n            _ => trow(\"error\")\n        end\n        mapreduce(+, 1:length(Dp)) do i\n            π/4.0 *(x[i]*1e6)*Q[i]*(Dp[i]*1e-6)^2.0*1e6\n        end\n    end\nend\n\nf = get_domainfunction(complex(1.6, 0.01))","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"note that Qsca and Qabs are functions that return the scattering and absorption cross section from Mie theory for a given size parameter and refractive index. The domain function f is specialized for the refractive index n = 1.6 + 0.01i\". ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The matrix is obtained via","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Dp, N = lognormal([[100.0, 500.0, 1.4]], d1 = 100.0, d2 = 1000.0, bins = 8)\n\nA = designmatrix(s, q, g) \nb = A*N ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The matrix A is 6x8. N are the number concentrations of at each size (length 8). A*N produces 6 measurements corresponding to three beta_s and three beta_a values. ","category":"page"},{"location":"manual/#Notes-2","page":"Manual","title":"Notes","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"In this example 8 sizes are mapped to 6 observations. The setpoint s and query q domains are distinctly different in type.\nThe input and output tuples annotate data. The annotations are interpreted by the domain function to decide which value to compute (scattering or absorption).\nThis \"toy\" example illustrates the advantage of the domainmatrix discretization method. Only a few lines of declarative code are needed to compute converted to the domain matrix, even though the underlying model is reasonably complex. There is need to explicitly write out the equations to discretize the domain.","category":"page"},{"location":"manual/#Example-3","page":"Manual","title":"Example 3","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"This example is the implementation of the Baart (1982) kernel shown in the beginning.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"K(qs)=exp(qcos(s))","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"In this example, the domain s comprises the integration from 0pi","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"s = range(0, stop = π, length = 120) \nq = range(0, stop = π/2, length = 120) ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The forward model domain function is ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"function get_domainfunction(K)   \n    function f(node::Domain)\n        s, x, q = node.s, node.x, node.q\n        Δt = (maximum(s) - minimum(s))./length(x)\n        y = @_ mapfoldl(K(q, (_-0.5)*Δt) * x[_] * Δt, +, 1:length(x)) \n    end\nend","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The design matrix is obtained via","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"f = get_domainfunction((x,y) -> exp(x*cos(y)))\nA = designmatrix(s, q, f) ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Again, in this  the solution is knonw. Evaluating mathrmb=mathrmmathbfAx with mathrmmathrmx=sin(s) approximately yields 2sin(q)q. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"x = sin.(s)\nb1 = A*x \nb2 = 2.0.*sinh.(q)./q","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"And again b1 and b2 are close. Also A computed using the designmatrix function is the same than the explicit integration by quadrature. ","category":"page"},{"location":"manual/#Benchmarks","page":"Manual","title":"Benchmarks","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Systems up to a 1000 equations are unproblematic. The setup for much larger system slows down due to the approx O(n^2) (or worse) time complexity of the SVD and generalized SVD factorization of the design matrix. Larger systems require switching to SVD free algorithms, which are currently not supported by this package. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, TimerOutputs, MatrixDepot\n\nto = TimerOutput()\n\nfunction benchmark(n)\n    r = mdopen(\"shaw\", n, false)\n    A, x = r.A, r.x\n    y = A * x\n    b = y + 0.05y .* randn(n)\n    Ψ = setupRegularizationProblem(A, 2)\n    for i = 1:1\n        @timeit to \"Setup  (n = $n)\" setupRegularizationProblem(A, 2)\n        @timeit to \"Invert (n = $n)\" solve(Ψ, b)\n    end\nend\n\nmap(benchmark, [10, 100, 1000])\nshow(to)","category":"page"},{"location":"#RegularizationTools.jl","page":"Home","title":"RegularizationTools.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package to perform Tikhonov regularization for small to moderate size problems.","category":"page"},{"location":"","page":"Home","title":"Home","text":"RegularizationTools.jl bundles a set routines to compute the regularized  Tikhonov inverse using standard linear algebra techniques.  ","category":"page"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Computes the Tikhonov inverse solution with optional boundary constraints.\nComputes optimal regularization parameter using generalized cross validation or the L-curve.\nSolves problems with up to a 1000 equations.\nSupports zero, first, and second order regularization out of the box.\nSupports specifying an a-priori estimate of the solution.\nSupports user specified smoothing matrices.\nUser friendly interface.\nExtensive documentation.","category":"page"},{"location":"#About","page":"Home","title":"About","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tikhonv regularization is also known as Phillips-Twomey-Tikhonov regularization or ridge regression (see Hansen, 2000 for a review). The Web-of-Sciences database lists more than 4500 peer-reviewed publications mentioning \"Tikhonov regularization\" in the title or abstract, with a current publication rate of ≈350 new papers/year. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The first draft of this code was part of my DifferentialMobilityAnalyzers package. Unfortunately, the initial set of algorithms were too limiting and too slow. I needed a better set of regularization tools to work with, which is how this package came into existence. Consequently, the scope of the package is defined by my need to support data inversions for the DifferentialMobilityAnalyzers project. My research area is not on inverse methods and I currently do not intend to grow this package into something that goes much beyond the implemented algorithms. However, the code is a generic implementation of the Tikhonov method and might be useful to applied scientists who need to solve standard ill-posed inverse problems that arise in many disciplines. ","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package computes the regularized Tikhonov inverse rm x_lambda by solving the minimization problem ","category":"page"},{"location":"","page":"Home","title":"Home","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right ","category":"page"},{"location":"","page":"Home","title":"Home","text":"subject to the optional constraint rm x_lrm xrm x_u. Here rm x_lambda is the regularized estimate of rm x, leftlVert cdotrightrVert _2 is the Euclidean norm, rm bf L is the Tikhonov filter matrix, lambda is the regularization parameter, and rm x_0 is a vector of an a-priori guess of the solution. The initial guess can be taken to be rm x_0=0 if no a-priori information is known. The solve function searches for the optimal lambda and returns the inverse. Optionally, rm x_l and rm x_u can be used to impose boundary constraints on the solution rm x_lambda.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The following script is a minimalist example how to use this package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using RegularizationTools, MatrixDepot, Lazy\nusing Random #hide\n\n# This is a test problem for regularization methods\nr = mdopen(\"shaw\", 100, false)       # Load the \"shaw\" problem from MatrixDepot\nA, x  = r.A, r.x                     # A is size(100,100), x is length(100)\nRandom.seed!(716)  # hide\n\ny = A * x                            # y is the true response \nb = y + 0.2y .* randn(100)           # response with superimposed noise\nx₀ = 0.4x                            # some a-priori estimate x₀\n\n# Solve 2nd order Tikhonov inversion (L = uppertridiag(−1, 2, −1)) with intial guess x₀\nxλ = invert(A, b, Lₖx₀(2, x₀))\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot(y, b, x, xλ, x₀) # hide","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package can be installed from the Julia package prompt with","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add RegularizationTools","category":"page"},{"location":"","page":"Home","title":"Home","text":"The closing square bracket switches to the package manager interface and the add command installs the package and any missing dependencies. To return to the Julia REPL hit the delete key.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To load the package run","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using RegularizationTools","category":"page"},{"location":"","page":"Home","title":"Home","text":"For optimal performance, also install the Intel MKL linear algebra library.","category":"page"},{"location":"#Related-work","page":"Home","title":"Related work","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MultivariateStats: Implements ridge regression without a priori estimate and does not include tools to find the optimal regularization parameter.\nRegularizedLeastSquares: Implements optimization techniques for large-scale scale linear systems.","category":"page"},{"location":"#Author-and-Copyright","page":"Home","title":"Author and Copyright","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Markus Petters, Department of Marine, Earth, and Atmospheric Sciences, NC State University.","category":"page"}]
}
